# -*- coding: utf-8 -*-
"""ML_Project.ipynb
Performs logistic and 
Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16X3aexN5-zWxZcp9JgWrdP1BgFCkQyOo
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import collections

data = pd.read_csv('/content/tracks.csv') # read the data
# data.head(1)

data['duration_min'] = (data['duration_ms'])/1000/60  # convert time units
data.drop(['duration_ms'], axis=1, inplace = True)    # drop duration_ms

# freq = collections.Counter(data['artists'])
# top_100 = sorted(freq.items(), key=lambda x: x[1], reverse=True)[:101]
# for i in range(100):
#   top_100[i] = top_100[i][0]
# print(top_100)
# artists = data['artists']

X = data[['duration_min', 'explicit', 'release_date', 'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature']]
y = data['popularity']
X

"""[1, 2, 3, 5, 6, 7, 8, 11, 12, 13, 14]

[4, 3, 8, 9, 0, 1, 7, 14]



"""
## convert into numpy
X, y = X.to_numpy(), y.to_numpy()
_X, _y = [], []
for i in range(X.shape[0]):
  flag = True
  for j in range(X.shape[1]):
    if X[i][j] != X[i][j]:
      flag = False
  if flag:
    _X.append(X[i])
    _y.append(Y[i])
    # _artists.append(artists[i])
for i in range(len(_X)):
  _X[i][2] = int (str (_X[i][2])[:4])

X, y = np.array(_X), np.array(_y)
# artist = np.array(_artists)
print(X.shape)

# for j in range(X.shape[1]):
#   mean, mx, mn = 0, -1e18, 1e18
#   for i in range(X.shape[0]):
#     mean += X[i][j]/X.shape[0]
#     mx = max(mx, X[i][j])
#     mn = min(mn, X[i][j])
#   for i in range(X.shape[0]):
#     X[i][j] = (X[i][j]-mean)/(mx-mn)

# for i in range(100):
#   lst = []
#   for j in range(X.shape[0]):
#     lst.append([1 if artists[2] == top_100[i] else 0])
#   X = np.append(X, lst, axis=1)


## plot histograms of the columns
plt.hist(X[:, 0], range=[0, 10], bins=np.arange(0, 10, 0.01))
plt.show()
plt.savefig('duration-histogram')

plt.hist(X[:, 3], range=[0, 1], bins=np.arange(0, 1, 0.01))
plt.show()
plt.savefig('energy-histogram')

plt.hist(X[:, 6], range=[-60, 5], bins=np.arange(-60, 5, 0.1))
plt.show()
plt.savefig('loudness-histogram')

# plt.hist(X[:, 6], range=[0, 11], bins=np.arange(0, 10, 1))
# plt.show()
# plt.savefig('key-histogram')

import seaborn as sns
cr = data.corr()                # correlation matrix
plt.figure(figsize=(20, 12))
sns.heatmap(cr, annot=True)     # heatmap

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

"""#Linear Model"""

from sklearn import linear_model
from sklearn.model_selection import GridSearchCV

def get_loss(X_train, y_train, X_test, y_test):
  linear_reg = linear_model.LinearRegression()
  linear_reg.fit(X_train, y_train)
  return (1/X_test.shape[0])*np.sum(np.square(linear_reg.predict(X_test) - y_test))

def k_fold_cv(k: int, X: np.ndarray, y: np.ndarray):
  '''
  This method performs K-fold cross validation on the input samples with labels.
    Parameters:
      k (int): Value of K
      X (numpy array): samples
      y (numpy array): expected output/labels
      alpha (float): value of alpha in K-fold cross validation
  '''
  loss = 0
  for i in range(k):
    X_test, X_train, y_test, y_train = [], [], [], []
    for j in range(len(X)):
      if j >= i*k and j < (i+1)*k:
        X_test.append(X[j])
        y_test.append(Y[j])
      else:
        X_train.append(X[j])
        y_train.append(Y[j])
    X_train, X_test, y_train, y_test = np.array(X_train), np.array(X_test), np.array(y_train), np.array(y_test)
    loss += get_loss(X_train, y_train, X_test, y_test)
  return loss / k

min_error_features = []
min_error = 1e18
for i in range(1, 2 ** 15):
    features = []
    for j in range(15):
        if i & (2 ** j) == 0: continue
        features.append(j)
    if 11 < len(features) < 8:
      continue
    X_cur = X_train[:, features]
    loss_linear = k_fold_cv(5, X_cur, y_train)
    # print(loss_linear)
    if min_error > loss_linear:
        min_error = loss_linear
        min_error_features = features[:]

print(min_error, min_error_features)

def forward_feature_selection(X: np.ndarray, y:np.ndarray):
  loss = 1e18
  features = []
  while True:
    best_feature = -1
    for i in range(15):
      if i in features:
        continue
      features.append(i)
      X_cur = X[:, features]
      nloss = k_fold_cv(5, X_cur, y)
      if nloss < loss:
        best_feature = i
        loss = nloss
      features.pop()
    if best_feature == -1:
      break
    features.append(best_feature)
  return features

forward_feature_selection(X_train, y)

"""##Lasso Regression"""

def lasso_graph(X, y):
  loss_list = []
  rng = np.arange(1e-4, 1e-3, 1e-4)
  for alpha in rng:
    lasso_reg = GridSearchCV(linear_model.Lasso(), {'alpha' : [alpha]}, scoring='neg_root_mean_squared_error', cv=10)
    lasso_reg.fit(X, y)
    loss_list.append(-lasso_reg.best_score_)
  
  plt.plot(rng, loss_list)
  plt.show()

lasso_graph(X_train, y_train)

lasso_reg = GridSearchCV(linear_model.Lasso(max_iter=1e5), {'alpha' : np.arange(1e-5, 1e-3, 1e-5)}, scoring='neg_root_mean_squared_error', cv=10)
lasso_reg.fit(X_train, y_train)
print(lasso_reg.best_params_)
print(-lasso_reg.best_score_)
best_alpha_lasso = lasso_reg.best_params_['alpha']

lasso = linear_model.Lasso(alpha=best_alpha_lasso, max_iter=1e5)
lasso.fit(X_train, y_train)
loss_lasso = (1/X_test.shape[0])*np.sqrt(np.sum(np.square(lasso.predict(X_test) - y_test)))
loss_lasso

"""##Ridge Regression"""

def ridge_graph(X, y):
  loss_list = []
  rng = np.arange(1, 10, 1)
  for alpha in rng:
    ridge_reg = GridSearchCV(linear_model.Ridge(), {'alpha' : [alpha]}, scoring='neg_root_mean_squared_error', cv=10)
    ridge_reg.fit(X, y)
    loss_list.append(-ridge_reg.best_score_)
  
  plt.plot(rng, loss_list)
  plt.show()

ridge_graph(X_train, y_train)

ridge_reg = GridSearchCV(linear_model.Ridge(max_iter=1e5), {'alpha' : np.arange(1, 10, 1e-1)}, scoring='neg_root_mean_squared_error', cv=10)
ridge_reg.fit(X_train, y_train)
print(ridge_reg.best_params_)
print(-ridge_reg.best_score_)
best_alpha_ridge = ridge_reg.best_params_['alpha']

ridge = linear_model.Ridge(alpha=best_alpha_ridge, max_iter=1e5)
ridge.fit(X_train, y_train)
loss_ridge = (1/X_test.shape[0])*np.sqrt(np.sum(np.square(ridge.predict(X_test) - y_test)))
loss_ridge

"""##Linear Regression"""

class LinearRegression:
    def __init__(self, alpha, max_iter):
        self.theta = np.zeros((1, ))
        self.alpha = alpha
        self.max_iter = max_iter

    def fit(self, X: np.ndarray, y: np.ndarray, get_loss_curve: bool = False):
        for i in range(X.shape[0]):
            np.append(X[i], 1)
        self.theta = np.zeros((X.shape[1], ))
        m = X.shape[0]
        loss_list = []
        for i in np.arange(self.max_iter):
            h_theta = np.dot(X, self.theta)
            d_J_theta = (1 / m) * np.dot(X.T, h_theta - y)
            self.theta = self.theta - self.alpha * d_J_theta
            if get_loss_curve:
                J_theta = (1/(2*m))*np.sum(np.square(h_theta - y))
                loss_list.append(J_theta)
        if get_loss_curve:
            plt.plot(np.arange(self.max_iter), loss_list)
            plt.show()

    def predict(self, X: np.ndarray) -> np.ndarray:
        return np.dot(X, self.theta)

linear_reg = linear_model.LinearRegression()
linear_reg.fit(X_train, y_train)
loss_linear = (1/X_test.shape[0])*np.sqrt(np.sum(np.square(linear_reg.predict(X_test) - y_test)))
loss_linear